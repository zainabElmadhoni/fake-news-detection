{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GA-PYCcpBcTH"
   },
   "source": [
    "In this Notebook I streamed tweets from 10 different twitter accounts. 5 real and verified news publishing channels, and 5 satire parody twitter accounts. Twitter took down every account that was known to publish fake news and accounts that didn't follow the new policies of twitter (I recommend you to check out the following article https://analyticsindiamag.com/twitter-fake-news/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELTnCGIl9wJX"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "\n",
    "# These are the keys you get when creating a twitter developer account\n",
    "\n",
    "CONSUMER_KEY = ''\n",
    "CONSUMER_SECRET = ''\n",
    "ACCESS_TOKEN = ''\n",
    "ACCESS_SECRET = ''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# authorization of consumer key and consumer secret \n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) \n",
    "  \n",
    "# set access to user's access key and access secret  \n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET) \n",
    "  \n",
    "# calling the api to be able to access the tweets\n",
    "api = tweepy.API(auth) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zl_RKiUwD3PZ"
   },
   "source": [
    "For us to have a balanced dataset I streamed 3000 tweet per account. Going beyond 3000 tweets causes the twitter API to crash and it gives an abuse warning that results in suspending access to the API for a time period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_SuF5vQFTDD"
   },
   "source": [
    "The following code is similar to this one. I just changed the screen name for different news publishers, and put them in respective lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s442qAW09wJn"
   },
   "outputs": [],
   "source": [
    "list_cnn = list()\n",
    "c=0\n",
    "for status in tweepy.Cursor(api.user_timeline, screen_name='@cnn', tweet_mode=\"extended\").items():\n",
    "    list_cnn.append([status.id,\n",
    "                         status.full_text,\n",
    "                         status.created_at,\n",
    "                         status.source,\n",
    "                         status.favorite_count,\n",
    "                         status.retweet_count,0\n",
    "                         ])\n",
    "    c=c+1\n",
    "    if c==3000:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xidua0CO9wJ0"
   },
   "outputs": [],
   "source": [
    "list_bbc = list()\n",
    "c=0\n",
    "for status in tweepy.Cursor(api.user_timeline, screen_name='@BBCWorld', tweet_mode=\"extended\").items():\n",
    "    list_bbc.append([status.id,\n",
    "                         status.full_text,\n",
    "                         status.created_at,\n",
    "                         status.source,\n",
    "                         status.favorite_count,\n",
    "                         status.retweet_count,0\n",
    "                         ])\n",
    "    c=c+1\n",
    "    if c==3000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZU2X5-K9wJ-"
   },
   "outputs": [],
   "source": [
    "list_fox = list()\n",
    "c=0\n",
    "for status in tweepy.Cursor(api.user_timeline, screen_name='@FoxNews', tweet_mode=\"extended\").items():\n",
    "    list_fox.append([status.id,\n",
    "                         status.full_text,\n",
    "                         status.created_at,\n",
    "                         status.source,\n",
    "                         status.favorite_count,\n",
    "                         status.retweet_count,0\n",
    "                         ])\n",
    "    c=c+1\n",
    "    if c==3000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rX-ByDR39wKJ"
   },
   "outputs": [],
   "source": [
    "list_euro = list()\n",
    "c=0\n",
    "for status in tweepy.Cursor(api.user_timeline, screen_name='@euronews', tweet_mode=\"extended\").items():\n",
    "    list_euro.append([status.id,\n",
    "                         status.full_text,\n",
    "                         status.created_at,\n",
    "                         status.source,\n",
    "                         status.favorite_count,\n",
    "                         status.retweet_count,0\n",
    "                         ])\n",
    "    c=c+1\n",
    "    if c==3000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88MNVNgZ9wKV"
   },
   "outputs": [],
   "source": [
    "list_aj = list()\n",
    "c=0\n",
    "for status in tweepy.Cursor(api.user_timeline, screen_name='@AJEnglish', tweet_mode=\"extended\").items():\n",
    "    list_aj.append([status.id,\n",
    "                         status.full_text,\n",
    "                         status.created_at,\n",
    "                         status.source,\n",
    "                         status.favorite_count,\n",
    "                         status.retweet_count,0\n",
    "                         ])\n",
    "    c=c+1\n",
    "    if c==3000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wBp8iT6cFzrE"
   },
   "source": [
    "The next 5 lists are from the parody accounts. I chose the most popular and active ones from the list of satirical news on wikipedia (link to the list https://en.wikipedia.org/wiki/List_of_satirical_news_websites)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trxBdAwc9wKj"
   },
   "outputs": [],
   "source": [
    "list_onion = list()\n",
    "c=0\n",
    "for status in tweepy.Cursor(api.user_timeline, screen_name='@TheOnion', tweet_mode=\"extended\").items():\n",
    "    list_onion.append([status.id,\n",
    "                         status.full_text,\n",
    "                         status.created_at,\n",
    "                         status.source,\n",
    "                         status.favorite_count,\n",
    "                         status.retweet_count,0\n",
    "                         ])\n",
    "    c=c+1\n",
    "    if c==3000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPT55tY_9wKu"
   },
   "outputs": [],
   "source": [
    "list_bee = list()\n",
    "c=0\n",
    "for status in tweepy.Cursor(api.user_timeline, screen_name='@TheBabylonBee', tweet_mode=\"extended\").items():\n",
    "    list_bee.append([status.id,\n",
    "                         status.full_text,\n",
    "                         status.created_at,\n",
    "                         status.source,\n",
    "                         status.favorite_count,\n",
    "                         status.retweet_count,0\n",
    "                         ])\n",
    "    c=c+1\n",
    "    if c==3000:\n",
    "        break       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZaKyHlU9wK8"
   },
   "outputs": [],
   "source": [
    "list_hole = list()\n",
    "c=0\n",
    "for status in tweepy.Cursor(api.user_timeline, screen_name='@ClickHole', tweet_mode=\"extended\").items():\n",
    "    list_hole.append([status.id,\n",
    "                         status.full_text,\n",
    "                         status.created_at,\n",
    "                         status.source,\n",
    "                         status.favorite_count,\n",
    "                         status.retweet_count,0\n",
    "                         ])\n",
    "    c=c+1\n",
    "    if c==3000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dh_RIe0s9wLF"
   },
   "outputs": [],
   "source": [
    "list_beaver = list()\n",
    "c=0\n",
    "for status in tweepy.Cursor(api.user_timeline, screen_name='@TheBeaverton', tweet_mode=\"extended\").items():\n",
    "    list_beaver.append([status.id,\n",
    "                         status.full_text,\n",
    "                         status.created_at,\n",
    "                         status.source,\n",
    "                         status.favorite_count,\n",
    "                         status.retweet_count,0\n",
    "                         ])\n",
    "    c=c+1\n",
    "    if c==3000:\n",
    "        break\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HcxXTr-09wLO"
   },
   "outputs": [],
   "source": [
    "list_reduct = list()\n",
    "c=0\n",
    "for status in tweepy.Cursor(api.user_timeline, screen_name='@Reductress', tweet_mode=\"extended\").items():\n",
    "    list_reduct.append([status.id,\n",
    "                         status.full_text,\n",
    "                         status.created_at,\n",
    "                         status.source,\n",
    "                         status.favorite_count,\n",
    "                         status.retweet_count,0\n",
    "                         ])\n",
    "    c=c+1\n",
    "    if c==3000:\n",
    "        break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gxo1xqg_HT8O"
   },
   "source": [
    "I find using dataframes to store data makes it accessible and easy to use. I created dataframes for each account with the same labels and data that I found to be the most important. \n",
    "The last column 'class' is the one I gave 0 for every list as you can see above. I will changed it to 1 for the fake news tweets later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkr0IrGK9wLZ"
   },
   "outputs": [],
   "source": [
    "df_cnn= pd.DataFrame(list_cnn,columns = ['id' , 'text', 'date','source','likes','retweets','class'])\n",
    "\n",
    "df_bbc= pd.DataFrame(list_bbc,columns = ['id' , 'text', 'date','source','likes','retweets','class'])\n",
    "\n",
    "df_fox= pd.DataFrame(list_fox,columns = ['id' , 'text', 'date','source','likes','retweets','class'])\n",
    "\n",
    "df_euro= pd.DataFrame(list_euro,columns = ['id' , 'text', 'date','source','likes','retweets','class'])\n",
    "\n",
    "df_aj= pd.DataFrame(list_aj,columns = ['id' , 'text', 'date','source','likes','retweets','class'])\n",
    "\n",
    "df_onion= pd.DataFrame(list_onion,columns = ['id' , 'text', 'date','source','likes','retweets','class'])\n",
    "\n",
    "df_bee= pd.DataFrame(list_bee,columns = ['id' , 'text', 'date','source','likes','retweets','class'])\n",
    "\n",
    "df_hole= pd.DataFrame(list_hole,columns = ['id' , 'text', 'date','source','likes','retweets','class'])\n",
    "\n",
    "df_beaver= pd.DataFrame(list_beaver,columns = ['id' , 'text', 'date','source','likes','retweets','class'])\n",
    "\n",
    "df_reduct= pd.DataFrame(list_reduct,columns = ['id' , 'text', 'date','source','likes','retweets','class'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EKhfS44YJP5n"
   },
   "source": [
    "Here I just combined fakes news tweets in the same dataframe 'df_fake'. And did the same with the real ones 'df_real'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_7hQ0vN9wLr"
   },
   "outputs": [],
   "source": [
    "df_real = pd.concat([df_cnn, df_bbc, df_fox,df_euro,df_aj])\n",
    "df_fake = pd.concat([df_onion, df_bee, df_hole,df_beaver,df_reduct])\n",
    "print (df_real.info())\n",
    "print(df_fake.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6qiv7UJJgD8"
   },
   "source": [
    "Here I changes the 'class' column for fake news to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efLV8otc9wL9"
   },
   "outputs": [],
   "source": [
    "df_fake[\"class\"].replace({0: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "erG0UueVJqXE"
   },
   "source": [
    "And here I just combined both real and fake news into one dataframe in order to save them in a file to work on later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRfAbNxJ9wMG"
   },
   "outputs": [],
   "source": [
    "data = pd.concat([df_real, df_fake])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aoJGVDJL9wMP"
   },
   "outputs": [],
   "source": [
    "# Reset index \n",
    "data.reset_index(drop=True,inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ImL91MFXKb0e"
   },
   "source": [
    "I chose to save the data to a pickle file because it is faster to upload data from a pickle file than other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXUlbSvd9wM_"
   },
   "outputs": [],
   "source": [
    "data.to_pickle(\"./twitter_data.pkl\") \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Part1. Streaming Tweets (Real and Fake).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
