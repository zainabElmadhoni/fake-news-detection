{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PLkuoakRZFa"
   },
   "source": [
    "In this notebook I applied different Machine Learning methods (SVM, Decisions Trees, Naive Bayes) on the data I streamed in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "N8Gy2o_uRO-Z",
    "outputId": "d60d8ef4-b204-428b-e672-44e6b4ccb4fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zainab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zainab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pcsLz3xFjwjK"
   },
   "source": [
    "I started by uploading the data into a dataframe, and below are all the informations about the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "colab_type": "code",
    "id": "nIS5NQ_bRO-g",
    "outputId": "a2f943f1-cc67-4994-f7b5-ed8d6c29a805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 7 columns):\n",
      "id          30000 non-null int64\n",
      "text        30000 non-null object\n",
      "date        30000 non-null datetime64[ns]\n",
      "source      30000 non-null object\n",
      "likes       30000 non-null int64\n",
      "retweets    30000 non-null int64\n",
      "class       30000 non-null int64\n",
      "dtypes: datetime64[ns](1), int64(4), object(2)\n",
      "memory usage: 1.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1287859173556920327</td>\n",
       "      <td>Rafale: First French fighter jets head to Indi...</td>\n",
       "      <td>2020-07-27 21:15:28</td>\n",
       "      <td>True Anthem</td>\n",
       "      <td>164</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1214207516726513672</td>\n",
       "      <td>Anti-Wall Extremist The Kool-Aid Man Leads Cam...</td>\n",
       "      <td>2020-01-06 15:30:05</td>\n",
       "      <td>Buffer</td>\n",
       "      <td>3836</td>\n",
       "      <td>704</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1288034661155901440</td>\n",
       "      <td>A video featuring a group of doctors making fa...</td>\n",
       "      <td>2020-07-28 08:52:48</td>\n",
       "      <td>SocialFlow</td>\n",
       "      <td>2073</td>\n",
       "      <td>912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1222515270708801544</td>\n",
       "      <td>Bell CEO’s mental health dramatically improves...</td>\n",
       "      <td>2020-01-29 13:42:08</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>822</td>\n",
       "      <td>612</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1248618917871812610</td>\n",
       "      <td>Lots of people are taking part in WHO’s #SafeH...</td>\n",
       "      <td>2020-04-10 14:28:42</td>\n",
       "      <td>Wildmoka</td>\n",
       "      <td>43</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1244737188908208128</td>\n",
       "      <td>New York City Health Officials Board Up Sun To...</td>\n",
       "      <td>2020-03-30 21:24:06</td>\n",
       "      <td>SocialFlow</td>\n",
       "      <td>932</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1225622721758998528</td>\n",
       "      <td>.@Lin_Manuel: “Holy hell, I wrote one musical ...</td>\n",
       "      <td>2020-02-07 03:30:02</td>\n",
       "      <td>Buffer</td>\n",
       "      <td>5776</td>\n",
       "      <td>488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1252215192576032768</td>\n",
       "      <td>Failure Now An Option https://t.co/wFtITBA6PZ ...</td>\n",
       "      <td>2020-04-20 12:39:01</td>\n",
       "      <td>Sprout Social</td>\n",
       "      <td>3471</td>\n",
       "      <td>427</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1283624204336857090</td>\n",
       "      <td>A Pennsylvania couple didn't know they had hou...</td>\n",
       "      <td>2020-07-16 04:47:13</td>\n",
       "      <td>SocialFlow</td>\n",
       "      <td>329</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1281400157330984960</td>\n",
       "      <td>California Attorney General Xavier Becerra ann...</td>\n",
       "      <td>2020-07-10 01:29:39</td>\n",
       "      <td>SocialFlow</td>\n",
       "      <td>1048</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1287859173556920327  Rafale: First French fighter jets head to Indi...   \n",
       "1  1214207516726513672  Anti-Wall Extremist The Kool-Aid Man Leads Cam...   \n",
       "2  1288034661155901440  A video featuring a group of doctors making fa...   \n",
       "3  1222515270708801544  Bell CEO’s mental health dramatically improves...   \n",
       "4  1248618917871812610  Lots of people are taking part in WHO’s #SafeH...   \n",
       "5  1244737188908208128  New York City Health Officials Board Up Sun To...   \n",
       "6  1225622721758998528  .@Lin_Manuel: “Holy hell, I wrote one musical ...   \n",
       "7  1252215192576032768  Failure Now An Option https://t.co/wFtITBA6PZ ...   \n",
       "8  1283624204336857090  A Pennsylvania couple didn't know they had hou...   \n",
       "9  1281400157330984960  California Attorney General Xavier Becerra ann...   \n",
       "\n",
       "                 date           source  likes  retweets  class  \n",
       "0 2020-07-27 21:15:28      True Anthem    164        59      0  \n",
       "1 2020-01-06 15:30:05           Buffer   3836       704      1  \n",
       "2 2020-07-28 08:52:48       SocialFlow   2073       912      0  \n",
       "3 2020-01-29 13:42:08  Twitter Web App    822       612      1  \n",
       "4 2020-04-10 14:28:42         Wildmoka     43        16      0  \n",
       "5 2020-03-30 21:24:06       SocialFlow    932       141      1  \n",
       "6 2020-02-07 03:30:02           Buffer   5776       488      1  \n",
       "7 2020-04-20 12:39:01    Sprout Social   3471       427      1  \n",
       "8 2020-07-16 04:47:13       SocialFlow    329        82      0  \n",
       "9 2020-07-10 01:29:39       SocialFlow   1048       222      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"./twitter_data.pkl\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df.info()\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BSX8RMsokAHM"
   },
   "source": [
    " <br> Before applying anything to the dataframe I wanted to see how many of the 30000 tweets are corona virus related by looking for the words \"corona\" or \"covid\" in the tweet text like below.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qedkyA1LRO-n",
    "outputId": "ae7126c2-e71a-4fdb-d953-e9170b3f4a86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3949\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for i in range(0, len(df)):\n",
    "    t=df['text'][i]\n",
    "    t=t.lower()\n",
    "    if t.find('corona')!=-1 or t.find('covid')!=-1: \n",
    "        c=c+1   \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4tl2aQrqlL2i"
   },
   "source": [
    "Since only 3949 tweets are covid related, which is about 13% of the data. I decided to use them as the testing test and the other tweets as the training set.\n",
    "I had the idea to see if Machine Learning models can differentiate between real and fake news when it comes to a new topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zk-kNiLHUDCs"
   },
   "outputs": [],
   "source": [
    "corona_tweets = list()\n",
    "for i in range(0, len(df)):\n",
    "    t=df['text'][i]\n",
    "    t=t.lower()\n",
    "    if t.find('corona')!=-1 or t.find('covid')!=-1: \n",
    "        corona_tweets.append([df['id'][i],\n",
    "                         df['text'][i],\n",
    "                         df['date'][i],\n",
    "                         df['source'][i],\n",
    "                         df['likes'][i],\n",
    "                         df['retweets'][i],\n",
    "                         df['class'][i]\n",
    "                         ])\n",
    "        df=df.drop(i)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df_ct= pd.DataFrame(corona_tweets,columns = ['id' , 'text', 'date','source','likes','retweets','class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "colab_type": "code",
    "id": "YRW6CFUaUMI8",
    "outputId": "f58e00e6-1bed-475a-bc92-509b2f217007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26051 entries, 0 to 26050\n",
      "Data columns (total 7 columns):\n",
      "id          26051 non-null int64\n",
      "text        26051 non-null object\n",
      "date        26051 non-null datetime64[ns]\n",
      "source      26051 non-null object\n",
      "likes       26051 non-null int64\n",
      "retweets    26051 non-null int64\n",
      "class       26051 non-null int64\n",
      "dtypes: datetime64[ns](1), int64(4), object(2)\n",
      "memory usage: 1.4+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3949 entries, 0 to 3948\n",
      "Data columns (total 7 columns):\n",
      "id          3949 non-null int64\n",
      "text        3949 non-null object\n",
      "date        3949 non-null datetime64[ns]\n",
      "source      3949 non-null object\n",
      "likes       3949 non-null int64\n",
      "retweets    3949 non-null int64\n",
      "class       3949 non-null int64\n",
      "dtypes: datetime64[ns](1), int64(4), object(2)\n",
      "memory usage: 216.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "df_ct.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LlV9qW8dl3Wa"
   },
   "source": [
    "\n",
    "Here I separated each sentence from the next sentences to deal with them individually. Then I remove UN-important words from each sentence. After that I used stemming ( Returns each word to its origin) then created a new column in both dataframes called 'cleaned', which contains the new cleaned tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "30u28KtORO-t"
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "words = stopwords.words(\"english\")\n",
    "\n",
    "df['cleaned'] = df['text'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "df_ct['cleaned'] = df_ct['text'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wv8p_tlZRO-3"
   },
   "outputs": [],
   "source": [
    "# Shuffling the data before applying ML algorithms \n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df_ct = df_ct.sample(frac=1).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> The 'fit_transform' function learns the vocabulary dictionary and returns document-term matrix. I used 'transform' function for the testing Set in order to have matching dimensions; otherwise you get an error </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUp2ESq6zOhE"
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X_train = cv.fit_transform(df['cleaned'])  \n",
    "X_test = cv.transform(df_ct['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIQ-enGRRO_P"
   },
   "outputs": [],
   "source": [
    "#LinearSVC is the linear classification method of SVM in sklearn library\n",
    "svc = LinearSVC().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "uZb3Xl3Cd2t2",
    "outputId": "5a71e883-5aed-4083-ddc7-2da9d24d5b97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     11566\n",
      "           1       1.00      1.00      1.00     14485\n",
      "\n",
      "    accuracy                           1.00     26051\n",
      "   macro avg       1.00      1.00      1.00     26051\n",
      "weighted avg       1.00      1.00      1.00     26051\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[11566     0]\n",
      " [    2 14483]]\n",
      "Accuracy:  0.9999232275152585\n"
     ]
    }
   ],
   "source": [
    "# Calculating the confusion matrix and accuracy score of the training Set\n",
    "pred= svc.predict(X_train)\n",
    "print(classification_report(y_train,pred))\n",
    "print()\n",
    "print('Confusion Matrix: \\n',confusion_matrix(y_train,pred))\n",
    "print('Accuracy: ',accuracy_score(y_train,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "K9YKo5Nfd14N",
    "outputId": "8c592d2e-06ec-4638-caa6-2b630fe674af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      3434\n",
      "           1       0.68      0.89      0.77       515\n",
      "\n",
      "    accuracy                           0.93      3949\n",
      "   macro avg       0.83      0.91      0.87      3949\n",
      "weighted avg       0.94      0.93      0.94      3949\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[3221  213]\n",
      " [  58  457]]\n",
      "Accuracy:  0.9313750316535832\n"
     ]
    }
   ],
   "source": [
    "# Calculating the confusion matrix and accuracy score of the testing Set\n",
    "pred= svc.predict(X_test)\n",
    "print(classification_report(y_test,pred))\n",
    "print()\n",
    "print('Confusion Matrix: \\n',confusion_matrix(y_test,pred))\n",
    "print('Accuracy: ',accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqBeeqpGt4Ot"
   },
   "source": [
    "As you can see I got 99% accuracy on training, and 93% on testing when using SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aCDPiaDzynp9"
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(criterion='gini',splitter='random',max_features='auto').fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "MOChh0sSdGeR",
    "outputId": "3e4e584d-b448-4c02-a2d6-2f3603fb4aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     11566\n",
      "           1       1.00      1.00      1.00     14485\n",
      "\n",
      "    accuracy                           1.00     26051\n",
      "   macro avg       1.00      1.00      1.00     26051\n",
      "weighted avg       1.00      1.00      1.00     26051\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[11566     0]\n",
      " [    0 14485]]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
    "pred= tree.predict(X_train)\n",
    "print(classification_report(y_train,pred))\n",
    "print()\n",
    "print('Confusion Matrix: \\n',confusion_matrix(y_train,pred))\n",
    "print('Accuracy: ',accuracy_score(y_train,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "CBklIXtrdZTp",
    "outputId": "e7cb43eb-3bba-440f-de8a-1a84e74a5540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.78      0.86      3434\n",
      "           1       0.35      0.80      0.49       515\n",
      "\n",
      "    accuracy                           0.78      3949\n",
      "   macro avg       0.66      0.79      0.67      3949\n",
      "weighted avg       0.88      0.78      0.81      3949\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[2663  771]\n",
      " [ 101  414]]\n",
      "Accuracy:  0.7791846036971385\n"
     ]
    }
   ],
   "source": [
    "pred= tree.predict(X_test)\n",
    "print(classification_report(y_test,pred))\n",
    "print()\n",
    "print('Confusion Matrix: \\n',confusion_matrix(y_test,pred))\n",
    "print('Accuracy: ',accuracy_score(y_test,pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JgGfSrTqueMk"
   },
   "source": [
    "When comes to the DecisionTrees the training accuracy is 100% yet the training accuracy is 77%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RuhdDpmBRPAS"
   },
   "outputs": [],
   "source": [
    "naive = naive_bayes.MultinomialNB().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "armfgqm6RPAY",
    "outputId": "d344bf30-99a4-4d72-c96b-f44e5630f16c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97     11566\n",
      "           1       0.97      0.98      0.98     14485\n",
      "\n",
      "    accuracy                           0.97     26051\n",
      "   macro avg       0.98      0.97      0.97     26051\n",
      "weighted avg       0.97      0.97      0.97     26051\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[11179   387]\n",
      " [  268 14217]]\n",
      "Accuracy:  0.9748570112471691\n"
     ]
    }
   ],
   "source": [
    "pred= naive.predict(X_train)\n",
    "print(classification_report(y_train,pred))\n",
    "print()\n",
    "print('Confusion Matrix: \\n',confusion_matrix(y_train,pred))\n",
    "print('Accuracy: ',accuracy_score(y_train,pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "YbRy8sodRPAe",
    "outputId": "358ecc54-84a5-4a39-ad31-cc12506e5568",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97      3434\n",
      "           1       0.72      0.88      0.79       515\n",
      "\n",
      "    accuracy                           0.94      3949\n",
      "   macro avg       0.85      0.91      0.88      3949\n",
      "weighted avg       0.95      0.94      0.94      3949\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[3262  172]\n",
      " [  63  452]]\n",
      "Accuracy:  0.9404912636110407\n"
     ]
    }
   ],
   "source": [
    "pred= classifier.predict(X_test)\n",
    "print(classification_report(y_test,pred))\n",
    "print()\n",
    "print('Confusion Matrix: \\n',confusion_matrix(y_test,pred))\n",
    "print('Accuracy: ',accuracy_score(y_test,pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-QO9Cv-N0f75"
   },
   "source": [
    "For Naive Bayes we have 97% accuracy for training and 94% accuracy for testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Um05HsVR0o2v"
   },
   "source": [
    "Even though SVM does better when it comes to the training data. Naive Bayes out performs it in the testing data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Part2. Machine Learning Methods (SVM, Decision Trees, Naive Bayes).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
